services:
  vllm:
image: vllm/vllm-openai:latest
command: [
  "--model", "Qwen/Qwen3-VL-4B-Instruct",  # Or "Qwen/Qwen3-VL-2B-Instruct" for even faster
  "--served-model-name", "gemma3",
  "--host", "0.0.0.0",
  "--port", "9000",
  "--gpu-memory-utilization", "0.97",  # Max it out safely
  "--tensor-parallel-size", "1",
  "--max-model-len", "32768",          # Plenty for long docs/images
  "--disable-log-requests",
  "--dtype", "bfloat16",
  "--enable-chunked-prefill",
  "--enable-prefix-caching",
  "--max-num-batched-tokens", "32768", # Huge boostâ€”test up from here
  "--max-num-seqs", "512",             # High concurrency
  "--chat-template-content-format", "openai",
  "--enable-auto-tool-choice",
  "--tool-call-parser", "hermes",
  "--limit-mm-per-prompt", '{"image": 10, "video": 0}']    
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    networks:
      - app-network
    restart: unless-stopped

  nginx:
    image: nginx:latest
    ports:
      - "80:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
    depends_on:
      - vllm
    networks:
      - app-network
    restart: unless-stopped

networks:
  app-network:
    driver: bridge